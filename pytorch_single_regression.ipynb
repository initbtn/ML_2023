{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d09a8d0fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data=torch.FloatTensor([[3],[5],[7],[9],[11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros(1, requires_grad=True)\n",
    "b=torch.zeros(1, requires_grad=True)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설 hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x_data*w+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((t_data-y)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법으로 w, b 업데이트하기 위한 최적화 표현식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([w,b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 w: 0.500000, b:0.140 Cost:57.000000\n",
      "Epoch  100/2000 w: 2.081501, b:0.706 Cost:0.015866\n",
      "Epoch  200/2000 w: 2.058087, b:0.790 Cost:0.008060\n",
      "Epoch  300/2000 w: 2.041400, b:0.851 Cost:0.004094\n",
      "Epoch  400/2000 w: 2.029507, b:0.893 Cost:0.002080\n",
      "Epoch  500/2000 w: 2.021030, b:0.924 Cost:0.001056\n",
      "Epoch  600/2000 w: 2.014988, b:0.946 Cost:0.000537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  700/2000 w: 2.010682, b:0.961 Cost:0.000273\n",
      "Epoch  800/2000 w: 2.007614, b:0.973 Cost:0.000138\n",
      "Epoch  900/2000 w: 2.005426, b:0.980 Cost:0.000070\n",
      "Epoch 1000/2000 w: 2.003868, b:0.986 Cost:0.000036\n",
      "Epoch 1100/2000 w: 2.002757, b:0.990 Cost:0.000018\n",
      "Epoch 1200/2000 w: 2.001965, b:0.993 Cost:0.000009\n",
      "Epoch 1300/2000 w: 2.001401, b:0.995 Cost:0.000005\n",
      "Epoch 1400/2000 w: 2.000998, b:0.996 Cost:0.000002\n",
      "Epoch 1500/2000 w: 2.000712, b:0.997 Cost:0.000001\n",
      "Epoch 1600/2000 w: 2.000508, b:0.998 Cost:0.000001\n",
      "Epoch 1700/2000 w: 2.000362, b:0.999 Cost:0.000000\n",
      "Epoch 1800/2000 w: 2.000258, b:0.999 Cost:0.000000\n",
      "Epoch 1900/2000 w: 2.000184, b:0.999 Cost:0.000000\n",
      "Epoch 2000/2000 w: 2.000131, b:1.000 Cost:0.000000\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    y=x_data*w+b\n",
    "    cost=torch.mean((y-t_data)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print('Epoch {:4d}/{} w: {:3f}, b:{:.3f} Cost:{:.6f}'.format(epoch, nb_epochs, w.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient를 0으로 초기화 \n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용함수 계산\n",
    "```python\n",
    "cost.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w,b 업데이트 \n",
    "```python\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Hypothesis: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>) Cost: 29661.80078125 W: tensor([[0.0294],\n",
      "        [0.0294],\n",
      "        [0.0297]], requires_grad=True) b: 0.00034199998481199145\n",
      "Epoch: 100 Hypothesis: tensor([[151.1122],\n",
      "        [181.6338],\n",
      "        [178.9626],\n",
      "        [194.8861],\n",
      "        [138.5426]], grad_fn=<AddBackward0>) Cost: 5.2780046463012695 W: tensor([[0.6614],\n",
      "        [0.6595],\n",
      "        [0.6685]], requires_grad=True) b: 0.007699820678681135\n",
      "Epoch: 200 Hypothesis: tensor([[152.7846],\n",
      "        [183.6522],\n",
      "        [180.9471],\n",
      "        [197.0474],\n",
      "        [140.0840]], grad_fn=<AddBackward0>) Cost: 1.6195074319839478 W: tensor([[0.6689],\n",
      "        [0.6659],\n",
      "        [0.6757]], requires_grad=True) b: 0.0077964793890714645\n",
      "Epoch: 300 Hypothesis: tensor([[152.7989],\n",
      "        [183.6775],\n",
      "        [180.9678],\n",
      "        [197.0703],\n",
      "        [140.1051]], grad_fn=<AddBackward0>) Cost: 1.6119823455810547 W: tensor([[0.6696],\n",
      "        [0.6653],\n",
      "        [0.6758]], requires_grad=True) b: 0.007812554016709328\n",
      "Epoch: 400 Hypothesis: tensor([[152.7948],\n",
      "        [183.6807],\n",
      "        [180.9667],\n",
      "        [197.0694],\n",
      "        [140.1093]], grad_fn=<AddBackward0>) Cost: 1.6049730777740479 W: tensor([[0.6701],\n",
      "        [0.6647],\n",
      "        [0.6759]], requires_grad=True) b: 0.00782773457467556\n",
      "Epoch: 500 Hypothesis: tensor([[152.7905],\n",
      "        [183.6837],\n",
      "        [180.9655],\n",
      "        [197.0683],\n",
      "        [140.1135]], grad_fn=<AddBackward0>) Cost: 1.5979753732681274 W: tensor([[0.6707],\n",
      "        [0.6641],\n",
      "        [0.6759]], requires_grad=True) b: 0.007842915132641792\n",
      "Epoch: 600 Hypothesis: tensor([[152.7862],\n",
      "        [183.6867],\n",
      "        [180.9642],\n",
      "        [197.0672],\n",
      "        [140.1175]], grad_fn=<AddBackward0>) Cost: 1.5910309553146362 W: tensor([[0.6712],\n",
      "        [0.6635],\n",
      "        [0.6760]], requires_grad=True) b: 0.00785806030035019\n",
      "Epoch: 700 Hypothesis: tensor([[152.7820],\n",
      "        [183.6897],\n",
      "        [180.9630],\n",
      "        [197.0661],\n",
      "        [140.1216]], grad_fn=<AddBackward0>) Cost: 1.584143877029419 W: tensor([[0.6718],\n",
      "        [0.6629],\n",
      "        [0.6761]], requires_grad=True) b: 0.00787314772605896\n",
      "Epoch: 800 Hypothesis: tensor([[152.7777],\n",
      "        [183.6926],\n",
      "        [180.9617],\n",
      "        [197.0650],\n",
      "        [140.1256]], grad_fn=<AddBackward0>) Cost: 1.5772674083709717 W: tensor([[0.6724],\n",
      "        [0.6622],\n",
      "        [0.6761]], requires_grad=True) b: 0.00788823515176773\n",
      "Epoch: 900 Hypothesis: tensor([[152.7734],\n",
      "        [183.6956],\n",
      "        [180.9604],\n",
      "        [197.0638],\n",
      "        [140.1296]], grad_fn=<AddBackward0>) Cost: 1.5704048871994019 W: tensor([[0.6729],\n",
      "        [0.6616],\n",
      "        [0.6762]], requires_grad=True) b: 0.007903322577476501\n",
      "Epoch: 1000 Hypothesis: tensor([[152.7691],\n",
      "        [183.6985],\n",
      "        [180.9591],\n",
      "        [197.0627],\n",
      "        [140.1336]], grad_fn=<AddBackward0>) Cost: 1.5636171102523804 W: tensor([[0.6735],\n",
      "        [0.6610],\n",
      "        [0.6762]], requires_grad=True) b: 0.007918410003185272\n",
      "Epoch: 1100 Hypothesis: tensor([[152.7649],\n",
      "        [183.7014],\n",
      "        [180.9578],\n",
      "        [197.0616],\n",
      "        [140.1376]], grad_fn=<AddBackward0>) Cost: 1.5568805932998657 W: tensor([[0.6740],\n",
      "        [0.6604],\n",
      "        [0.6763]], requires_grad=True) b: 0.007933497428894043\n",
      "Epoch: 1200 Hypothesis: tensor([[152.7607],\n",
      "        [183.7043],\n",
      "        [180.9565],\n",
      "        [197.0605],\n",
      "        [140.1416]], grad_fn=<AddBackward0>) Cost: 1.5501492023468018 W: tensor([[0.6746],\n",
      "        [0.6598],\n",
      "        [0.6763]], requires_grad=True) b: 0.007948584854602814\n",
      "Epoch: 1300 Hypothesis: tensor([[152.7565],\n",
      "        [183.7072],\n",
      "        [180.9553],\n",
      "        [197.0594],\n",
      "        [140.1456]], grad_fn=<AddBackward0>) Cost: 1.5434629917144775 W: tensor([[0.6751],\n",
      "        [0.6592],\n",
      "        [0.6764]], requires_grad=True) b: 0.007963662967085838\n",
      "Epoch: 1400 Hypothesis: tensor([[152.7523],\n",
      "        [183.7101],\n",
      "        [180.9540],\n",
      "        [197.0583],\n",
      "        [140.1495]], grad_fn=<AddBackward0>) Cost: 1.5367997884750366 W: tensor([[0.6756],\n",
      "        [0.6586],\n",
      "        [0.6765]], requires_grad=True) b: 0.007978657260537148\n",
      "Epoch: 1500 Hypothesis: tensor([[152.7480],\n",
      "        [183.7129],\n",
      "        [180.9527],\n",
      "        [197.0571],\n",
      "        [140.1534]], grad_fn=<AddBackward0>) Cost: 1.530182123184204 W: tensor([[0.6762],\n",
      "        [0.6580],\n",
      "        [0.6765]], requires_grad=True) b: 0.007993652485311031\n",
      "Epoch: 1600 Hypothesis: tensor([[152.7439],\n",
      "        [183.7159],\n",
      "        [180.9515],\n",
      "        [197.0560],\n",
      "        [140.1574]], grad_fn=<AddBackward0>) Cost: 1.5236281156539917 W: tensor([[0.6767],\n",
      "        [0.6574],\n",
      "        [0.6766]], requires_grad=True) b: 0.00800864677876234\n",
      "Epoch: 1700 Hypothesis: tensor([[152.7398],\n",
      "        [183.7188],\n",
      "        [180.9503],\n",
      "        [197.0551],\n",
      "        [140.1614]], grad_fn=<AddBackward0>) Cost: 1.5170745849609375 W: tensor([[0.6773],\n",
      "        [0.6568],\n",
      "        [0.6766]], requires_grad=True) b: 0.00802364107221365\n",
      "Epoch: 1800 Hypothesis: tensor([[152.7356],\n",
      "        [183.7216],\n",
      "        [180.9490],\n",
      "        [197.0539],\n",
      "        [140.1653]], grad_fn=<AddBackward0>) Cost: 1.5105767250061035 W: tensor([[0.6778],\n",
      "        [0.6562],\n",
      "        [0.6767]], requires_grad=True) b: 0.008038635365664959\n",
      "Epoch: 1900 Hypothesis: tensor([[152.7314],\n",
      "        [183.7244],\n",
      "        [180.9477],\n",
      "        [197.0527],\n",
      "        [140.1691]], grad_fn=<AddBackward0>) Cost: 1.5040873289108276 W: tensor([[0.6784],\n",
      "        [0.6556],\n",
      "        [0.6767]], requires_grad=True) b: 0.008053629659116268\n",
      "Epoch: 2000 Hypothesis: tensor([[152.7274],\n",
      "        [183.7273],\n",
      "        [180.9465],\n",
      "        [197.0517],\n",
      "        [140.1731]], grad_fn=<AddBackward0>) Cost: 1.4976555109024048 W: tensor([[0.6789],\n",
      "        [0.6550],\n",
      "        [0.6768]], requires_grad=True) b: 0.008068623952567577\n"
     ]
    }
   ],
   "source": [
    "x1_data=torch.FloatTensor([[73., 80., 75.],\n",
    "            [93., 88., 93.],\n",
    "            [89., 91., 90.],\n",
    "            [96., 98., 100.],\n",
    "            [73., 66., 70.]])\n",
    "t1_data=torch.FloatTensor([[152.],[185.],[180.],[196.],[142.]])\n",
    "\n",
    "w1=torch.zeros((3,1), requires_grad=True)\n",
    "b1=torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer1=optim.SGD([w1,b1], lr=1e-6)\n",
    "nb2_epochs=2000\n",
    "for epoch in range(nb2_epochs+1):\n",
    "    y=x1_data.matmul(w1)+b1\n",
    "    cost=torch.mean((y-t1_data)**2)\n",
    "\n",
    "    optimizer1.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        # print('Epoch {:4d}/{} Hypothesis:{} Cost:{:.6f}'.format(epoch, nb2_epochs, y.squeeze().detach(), cost.item()))\n",
    "        print('Epoch:',epoch, 'Hypothesis:', y, 'Cost:', cost.item(), 'W:', w1 ,'b:',b1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
